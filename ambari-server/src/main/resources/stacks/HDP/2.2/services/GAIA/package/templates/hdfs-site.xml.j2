<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>{{hadoop_tmp_dir}}</value>
    <description>A base for other temporary directories.</description>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value>{{datanode_data_dirs}}</value>
    <description>Determines where on the local filesystem an DFS data node
    should store its blocks.  If this is a comma-delimited
    list of directories, then data will be stored in all named
    directories, typically on different devices.
    Directories that do not exist are ignored.
    </description>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>${hadoop.tmp.dir}/dfsnamespace</value>
    <description> Hadoop filesystem dir</description>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>${hadoop.tmp.dir}/dfsnamespace</value>
    <description> Hadoop filesystem dir</description>
  </property>

  <property>
    <name>dfs.namenode.edits.dir</name>
    <value>{{namenode_edits_dir}}</value>
    <description>the directory where store the edit log</description>
  </property>

  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:8081</value>
    <description>
    The datanode http server address and port.
    If the port is 0 then the server will start on a free port.
    </description>
  </property>

  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:9003</value>
    <description>The datanode server address and port for data transfer.</description>
  </property>

  <property>
    <name>dfs.datanode.ipc.address</name>
    <value>0.0.0.0:9010</value>
  </property>

  <property>
    <name>dfs.datanode.handler.count</name>
    <value>32</value>
    <description>The number of server threads for the datanode.</description>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value>256</value>
    <description>The number of server threads for the namenode.</description>
  </property>

  <property>
    <name>dfs.blocksize</name>
    <value>268435456</value>
    <description>
    HDFS blocksize of 512MB for large file-systems.
    </description>
  </property>

  <property>
    <name>io.file.buffer.size</name>
    <value>65536</value>
    <description>
    Size of read/write buffer used in SequenceFiles
    </description>
  </property>   

  <property>
    <name>hadoop.home</name>
    <value>{{hadoop_home}}</value>
  </property>

  <property>
    <name>fs.trash.interval</name>
    <value>20160</value>
    <description>Number of minutes between trash checkpoints.
    If zero, the trash feature is disabled.
    </description>
  </property>

  <property>
    <name>ipc.server.listen.queue.size</name>
    <value>512</value>
    <description>Indicates the length of the listen queue for servers accepting client connections.
    </description>
  </property>

  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>6144</value>
    <description> Specifies the maximum number of threads to use for transferring data in and out of the DN.</description>
  </property>

  <property>
    <name>dfs.blockreport.intervalMsec</name>
    <value>7200000</value>
    <description>Determines block reporting interval in milliseconds.</description>
  </property>

  <property>
    <name>dfs.datanode.du.reserved</name>
    <value>10737418240</value>
    <description>(10GB)Reserved space in bytes per volume. Always leave this much space free for non dfs use.
    </description>
  </property>

  <property>
    <name>dfs.replication</name>
    <value>3</value>
    <description>Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    </description>
  </property>

  <property>
    <name>dfs.https.enable</name>
    <value>false</value>
    <description>Decide if HTTPS(SSL) is supported on HDFS</description>
  </property>

  <property>
    <name>dfs.namenode.replication.considerLoad</name>
    <value>true</value>
    <description>Decide if chooseTarget considers the target's load or not</description>
  </property>

  <property>
    <name>dfs.permissions.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.heartbeat.interval</name>
    <value>10</value>
    <description>Determines datanode heartbeat intereval in seconds.</description>
  </property>

  <property>
    <name>dfs.namenode.safemode.threshold-pct</name>
    <value>0.9999f</value>
    <description>"0.99999f" is the default value.</description>
  </property>

  <property>
    <name>dfs.namenode.safemode.extension</name>
    <value>0</value>
    <description>Determines extension of safe mode in milliseconds after the threshold level is reached.</description>
  </property>

  <property>
    <name>dfs.datanode.balance.bandwidthPerSec</name>
    <value>20971520</value>
    <description>Determines the maximum amout of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. default value is "16MB"</description>
  </property>

  <property>
    <name>dfs.hosts.exclude</name>
    <value>${hadoop.home}/etc/hadoop/nodes_to_decommission</value>
  </property>

  <property>
    <name>dfs.namenode.decommission.interval</name>
    <value>300</value>
    <description>Namenode periodicity in seconds to check if decommission is complete.</description>
  </property>

  <property>
    <name>dfs.namenode.checkpoint.period</name>
    <value>5400</value>
    <description>3 hours, The number of seconds between two periodic checkpoints.</description>
  </property>

  <property>
    <name>dfs.namenode.checkpoint.txns</name>
    <value>20000000</value>
    <description>"1000000" is the default value. The secondary NameNode or CheckpointNode will create a checkpoint of the namespace every'dfs.naemnode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.perod' has expired.</description>
  </property>

  <property>
    <name>dfs.namenode.checkpoint.check.period</name>
    <value>300</value>
    <description> The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions</description>
  </property>

  <property>
    <name>dfs.namenode.checkpoint.max-retries</name>
    <value>3</value>
    <description>The SecondaryNameNode retries failed checkpointing. If the failure occurs while loading fsimage or replaying edits, the number of retries is limited by this variable. </description>
  </property>

  <property>
    <name>dfs.namenode.support.allow.format</name>
    <value>true</value>
    <description> Does HDFS namenode allow itself to be formatted ? You may consider setting this to false for any production cluster, to avoid any possibility of formatting a running DFS.</description>
  </property>

  <property>
    <name>dfs.support.append</name>
    <value>true</value>
    <description> Does HDFS allow appends to files?</description>
  </property>

  <!-- config for namenode ha -->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://{{yarn_cluster_id}}</value>
  </property>

  <property>
    <name>dfs.nameservices</name>
    <value>{{yarn_cluster_id}}</value>
  </property>

  <property>
    <name>dfs.ha.namenodes.{{yarn_cluster_id}}</name>
    <value>nn1,nn2</value>
    <description>enable HA mode, one nn is active while another is hot backup.</description>
  </property>

  <property>
    <name>dfs.namenode.rpc-address.{{yarn_cluster_id}}.nn1</name>
    <value>{{namenode_host}}:{{namenode_rpc_port}}</value>
    <description>RPC address that handles all clients requests.</description>
  </property>

  <property>
    <name>dfs.namenode.http-address.{{yarn_cluster_id}}.nn1</name>
    <value>{{namenode_host}}:{{namenode_http_port}}</value>
    <description>
    The address and the base port where the dfs namenode web ui will listen on.
    If the port is 0 then the server will start on a free port.
    </description>
  </property>

  <property>
    <name>dfs.namenode.rpc-address.{{yarn_cluster_id}}.nn2</name>
    <value>{{snamenode_host}}:{{snamenode_rpc_port}}</value>
  </property>

  <property>
    <name>dfs.namenode.http-address.{{yarn_cluster_id}}.nn2</name>
    <value>{{snamenode_host}}:{{snamenode_http_port}}</value>
    <description>
    The address and the base port where the dfs namenode web ui will listen on.
    If the port is 0 then the server will start on a free port.
    </description>
  </property>

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>{{journalnode_edits_dir}}</value>
    <description>A directory on shared storage between the multiple namenodes in an HA cluster.
      It should be left empty in a non-HA cluster.
    </description>
  </property>

  <property>
    <name>dfs.journalnode.rpc-address</name>
    <value>0.0.0.0:{{journalnode_rpc_port}}</value>
    <description> The JournalNode RPC server address and port </description>
  </property>

  <property>
    <name>dfs.journalnode.http-address</name>
    <value>0.0.0.0:{{journalnode_http_port}}</value>
    <description> The address and port the JournalNode web UI listens on. </description>
  </property>

  <property>
    <name>dfs.namenode.shared.edits.dir.{{yarn_cluster_id}}</name>
    <value>qjournal://{{journalnode_address}}/{{yarn_cluster_id}}</value>
  </property>

  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>shell(/bin/true)</value>
  </property>

  <property>
    <name>dfs.ha.fencing.ssh.connect-timeout</name>
    <value>10000</value>
    <description>SSH connection timeout, in milliseconds, to use with the builtin sshfence fencer.</description>
  </property>

  <property>
    <name>dfs.client.failover.proxy.provider.{{yarn_cluster_id}}</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>

  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>ha.zookeeper.quorum</name>
    <value>{{zk_urls}}</value>
    <description>zookeeper</description>
  </property>

  <property>
    <name>ha.zookeeper.session-timeout.ms</name>
    <value>60000</value>
    <description>The session timeout to use when the ZKFC connects to ZooKeeper</description>
  </property>

  <property>
    <name>ha.zookeeper.parent-znode</name>
    <value>/{{yarn_cluster_id}}-ha</value>
  </property>

  <property>
    <name>dfs.namenode.num.checkpoints.retained</name>
    <value>2</value>
    <description>The number of image checkpoint files that will be retained by the NameNode and Secondary NameNode in their storage directories. All edit logs necessary to recover an up-to-date namespace from the oldest retained checkpoint will also be retained.</description>
  </property>

  <property>
    <name>dfs.namenode.num.extra.edits.retained</name>
    <value>20000000</value>
    <description>The number of extra transactions which should be retained beyond what is minimally necessary for a NN restart. This can be useful for audit purposes or for an HA setup where a remote Standby Node may have been offline for some time and need to have a longer backlog of retained edits in order to start again. Typically each edit is on the order of a few hundreds of MBs or low GBs. NOTE: Fewer extra edits may be retained than value specified for this setting if doing so would mean that more segments would be retained than the number configured by dfs.namenode.max.extra.edits.segments.retained.</description>
  </property>

  <property>
    <name>dfs.namenode.max.extra.edits.segments.retained</name>
    <value>10000</value>
    <description>The maximum number of extra edit log segments which should be retained beyond what is minimally mecessary for a NN restart. When used in conjunction with dfs.namenode.num.extra.edits.retained, this configuration property serves to cap the number of extra edits files to a reasonable value.</description>
  </property>

  <property>
    <name>dfs.ha.log-roll.period</name>
    <value>120</value>
    <description>How often, in seconds, the StandbyNode should ask the active to roll edit logs. Since the StandbyNode only reads from finalized log segments, the StandbyNode will only be as up-to-date as how often the logs are rolled. Note that failover triggers a log roll so the SandbyNode will be up to date before it becomes active.</description>
  </property>

  <property>
    <name>dfs.ha.tail-edits.period</name>
    <value>60</value>
    <description>How often, in seconds, the StandbyNode should check for new finalized log segments in the shared edits log.</description>
  </property>
</configuration>


