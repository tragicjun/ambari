<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

    <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
    <!-- that are implied by Hadoop setup variables.                                                -->
    <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
    <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
    <!-- resource).                                                                                 -->

    <!-- Hive Execution Parameters -->
    <property>
        <name>mapred.reduce.tasks</name>
        <value>-1</value>
        <description>The default number of reduce tasks per job.  Typically set
            to a prime close to the number of available hosts.  Ignored when
            mapred.job.tracker is "local". Hadoop set this to 1 by default, whereas hive uses -1 as its default value.
            By setting this property to -1, Hive will automatically figure out what should be the number of reducers.
        </description>
    </property>

    <property>
        <name>hive.exec.reducers.bytes.per.reducer</name>
        <value>1000000000</value>
        <description>size per reducer.The default is 1G, i.e if the input size is 10G, it will use 10 reducers.</description>
    </property>

    <property>
        <name>hive.exec.reducers.max</name>
        <value>999</value>
        <description>max number of reducers will be used. If the one
            specified in the configuration parameter mapred.reduce.tasks is
            negative, hive will use this one as the max number of reducers when
            automatically determine number of reducers.</description>
    </property>

    <property>
        <name>hive.exec.scratchdir</name>
        <value>/tmp/tdw-${user.name}</value>
        <description>Scratch space for Hive jobs</description>
    </property>

    <property>
        <name>hive.test.mode</name>
        <value>false</value>
        <description>whether hive is running in test mode. If yes, it turns on sampling and prefixes the output tablename</description>
    </property>

    <property>
        <name>hive.test.mode.prefix</name>
        <value>test_</value>
        <description>if hive is running in test mode, prefixes the output table by this string</description>
    </property>
    <property>
        <name>hive.isSuccessful.retrytime</name>
        <value>5</value>
        <description>  retry time  of hive submit jobstatus</description>
    </property>
    <property>
        <name>hive.isSuccessful.waittime</name>
        <value>5000</value>
        <description> waittime 5 senconds of hive submit jobstatus  retry</description>
    </property>
    <property>
        <name>hive.optimize.cuberollup</name>
        <value>false</value>
        <description> whether to optimize the cube and rollup function </description>
    </property>
    <property>
        <name>hive.cuberollup.reduce</name>
        <value>false</value>
        <description> whether to open reduce and to extend the reduce task number to optimize the execute speed. Attention: open this may increase the number of reducers to several times. </description>
    </property>
    <property>
        <name>hive.cuberollup.reduce.skew</name>
        <value>true</value>
    <description> if the group by key is skew then open this rely on the hive.cuberollup.reduce</description>
    </property>
    <property>
        <name>hive.cuberollup.reduce.threshould</name>
        <value>1</value>
        <description> to adjust the number of reduces, rely on the hive.cuberollup.reduce </description>
    </property>
    <!-- If the input table is not bucketed, the denominator of the tablesample is determinied by the parameter below   -->
    <!-- For example, the following query:                                                                              -->
    <!--   INSERT OVERWRITE TABLE dest                                                                                  -->
    <!--   SELECT col1 from src                                                                                         -->
    <!-- would be converted to                                                                                          -->
    <!--   INSERT OVERWRITE TABLE test_dest                                                                             -->
    <!--   SELECT col1 from src TABLESAMPLE (BUCKET 1 out of 32 on rand(1))                                             -->
    <property>
        <name>hive.test.mode.samplefreq</name>
        <value>32</value>
        <description>if hive is running in test mode and table is not bucketed, sampling frequency</description>
    </property>

    <property>
        <name>hive.test.mode.nosamplelist</name>
        <value></value>
        <description>if hive is running in test mode, dont sample the above comma seperated list of tables</description>
    </property>

    <property>
        <name>hive.metastore.local</name>
        <value>true</value>
        <description>controls whether to connect to remove metastore server or open a new metastore server in Hive Client JVM</description>
    </property>

    <property>
        <name>hive.metastore.global.url</name>
        <value>jdbc:postgresql://{{pg_server_hosts}}:5432/global</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>
	<property>
        <name>hive.metastore.pbjar.url</name>
        <value>jdbc:postgresql://{{pg_server_hosts}}:5432/pbjar</value>
        <description>JDBC connect string for a pb jar JDBC metastore</description>
	</property>

    <property>
        <name>hive.metastore.user</name>
        <value>postgres</value>
        <description>username to use against metastore database</description>
    </property>

    <property>
        <name>hive.metastore.passwd</name>
        <value>postgres</value>
        <description>password to use against metastore database</description>
    </property>

	<property>
        <name>hive.metastore.router.buffer.enable</name>
        <value>true</value>
        <description>enable db router buffer or not</description>
	</property>
	<property>
        <name>hive.metastore.router.buffer.sync.time</name>
        <value>120</value>
        <description>route buffer sync time</description>
	</property>

    <property>
        <name>hive.metadata.usedistributetransaction</name>
        <value>false</value>
        <description>enable distribute transaction or not</description>
    </property>

    <property>
        <name>hive.metadata.conn.pool.enable</name>
        <value>true</value>
        <description>enable connect pool or not</description>
    </property>

    <property>
        <name>hive.metadata.conn.pool.activesize</name>
        <value>20</value>
        <description>active connect pool size </description>
    </property>

    <property>
        <name>datanucleus.validateTables</name>
        <value>false</value>
        <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
    </property>

    <!--<property>                                                                                                                                    -->
    <!--    <name>hive.aux.jars.path</name>                                                                                                           -->
    <!--    <value>file:///data/tdwadmin/tdwenv/qe/lib/hadoop-gpl-compression-0.1.0.jar,file:///data/tdwadmin/tdwenv/qe/lib/protobufRIO.jar</value>   -->
    <!--    <description></description>                                                                                                               -->
    <!--</property>     
                                                                                                                                -->
    <property>
        <name>datanucleus.validateColumns</name>
        <value>false</value>
        <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
    </property>

    <property>
        <name>datanucleus.validateConstraints</name>
        <value>false</value>
        <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
    </property>

    <property>
        <name>datanucleus.storeManagerType</name>
        <value>rdbms</value>
        <description>metadata store type</description>
    </property>

    <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>true</value>
        <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
    </property>

    <property>
        <name>datanucleus.autoStartMechanismMode</name>
        <value>checked</value>
        <description>throw exception if metadata tables are incorrect</description>
    </property>

    <property>
        <name>datanucleus.transactionIsolation</name>
        <value>repeatable-read</value>
        <description></description>
    </property>

    <property>
        <name>datanucleus.cache.level2</name>
        <value>true</value>
        <description>use a level 2 cache. turn this off if metadata is changed independently of hive metastore server</description>
    </property>

    <property>
        <name>datanucleus.cache.level2.type</name>
        <value>none</value>
        <description>SOFT=soft reference based cache, WEAK=weak reference based cache.</description>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/tdw/warehouse</value>
        <description>location of default database for the warehouse</description>
    </property>

    <property>
        <name>hive.metastore.connect.retries</name>
        <value>5</value>
        <description>Number of retries while opening a connection to metastore</description>
    </property>

    <property>
        <name>hive.metastore.rawstore.impl</name>
        <value>org.apache.hadoop.hive.metastore.JDBCStore</value>
        <description>Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database</description>
    </property>

    <property>
        <name>hive.map.aggr</name>
        <value>true</value>
        <description>Whether to use map-side aggregation in Hive Group By queries</description>
    </property>

    <property>
        <name>hive.groupby.skewindata</name>
        <value>false</value>
        <description>Whether there is skew in data to optimize group by queries</description>
    </property>

    <property>
        <name>hive.groupby.mapaggr.checkinterval</name>
        <value>100000</value>
        <description>Number of rows after which size of the grouping keys/aggregation classes is performed</description>
    </property>

    <property>
        <name>hive.mapred.local.mem</name>
        <value>0</value>
        <description>For local mode, memory of the mappers/reducers</description>
    </property>

    <property>
        <name>hive.map.aggr.hash.percentmemory</name>
        <value>0.2</value>
        <description>Portion of total memory to be used by map-side grup aggregation hash table</description>
    </property>

    <property>
        <name>hive.map.aggr.hash.min.reduction</name>
        <value>0.8</value>
        <description>Hash aggregation will be turned off if the ratio between hash
            table size and input rows is bigger than this number. Set to 1 to make sure
            hash aggregation is never turned off.</description>
    </property>

    <property>
        <name>hive.optimize.cp</name>
        <value>true</value>
        <description>Whether to enable column pruner</description>
    </property>

    <property>
        <name>hive.optimize.ppd</name>
        <value>true</value>
        <description>Whether to enable predicate pushdown</description>
    </property>

    <property>
        <name>hive.optimize.pruner</name>
        <value>true</value>
        <description>Whether to enable the new partition pruner which depends on predicate pushdown. If this is disabled,
            the old partition pruner which is based on AST will be enabled.</description>
    </property>

    <property>
        <name>hive.join.emit.interval</name>
        <value>1000</value>
        <description>How many rows in the right-most join operand Hive should buffer before emitting the join result. </description>
    </property>

    <property>
        <name>hive.mapred.mode</name>
        <value>nonstrict</value>
        <description>The mode in which the hive operations are being performed. In strict mode, some risky queries are not allowed to run</description>
    </property>

    <property>
        <name>hive.exec.script.maxerrsize</name>
        <value>100000</value>
        <description>Maximum number of bytes a script is allowed to emit to standard error (per map-reduce task). This prevents runaway scripts from filling logs partitions to capacity </description>
    </property>

    <property>
        <name>hive.exec.compress.output</name>
        <value>false</value>
        <description> This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
    </property>

    <property>
        <name>hive.exec.compress.intermediate</name>
        <value>false</value>
        <description> This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
    </property>

    <property>
        <name>hive.hwi.listen.host</name>
        <value>0.0.0.0</value>
        <description>This is the host address the Hive Web Interface will listen on</description>
    </property>

    <property>
        <name>hive.hwi.listen.port</name>
        <value>9999</value>
        <description>This is the port the Hive Web Interface will listen on</description>
    </property>

    <property>
        <name>hive.hwi.war.file</name>
        <value>${HIVE_HOME}/lib/hive-hwi.war</value>
        <description>This is the WAR file with the jsp content for Hive Web Interface</description>
    </property>

    <property>
        <name>hive.exec.pre.hooks</name>
        <value></value>
        <description>Pre Execute Hook for Tests</description>
    </property>

    <property>
        <name>hive.merge.mapfiles</name>
        <value>true</value>
        <description>Merge small files at the end of a map-only job</description>
    </property>

    <property>
        <name>hive.merge.mapredfiles</name>
        <value>false</value>
        <description>Merge small files at the end of any job(map only or map-reduce)</description>
    </property>

    <property>
        <name>hive.heartbeat.interval</name>
        <value>1000</value>
        <description>Send a heartbeat after this interval - used by mapjoin and filter operators</description>
    </property>

    <property>
        <name>hive.merge.size.per.task</name>
        <value>256000000</value>
        <description>Size of merged files at the end of the job</description>
    </property>

    <property>
        <name>hive.script.auto.progress</name>
        <value>false</value>
        <description>Whether Hive Tranform/Map/Reduce Clause should automatically send progress information to TaskTracker to avoid the task getting killed because of inactivity.  Hive sends progress information when the script is outputting to stderr.  This option removes the need of periodically producing stderr messages, but users should be cautious because this may prevent infinite loops in the scripts to be killed by TaskTracker.  </description>
    </property>

    <property>
        <name>hive.script.serde</name>
        <value>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</value>
        <description>The default serde for trasmitting input data to and reading output data from the user scripts. </description>
    </property>

    <property>
        <name>hive.script.recordreader</name>
        <value>org.apache.hadoop.hive.ql.exec.TextRecordReader</value>
        <description>The default record reader for reading data from the user scripts. </description>
    </property>

    <property>
        <name>hive.metastore.checkForRootUser</name>
        <value>true</value>
        <description>Added by BrantZhang for the initialization of root user. </description>
    </property>

    <property>
        <name>hadoop.hive.ql.parse.limitop</name>
        <value>1000</value>
        <description>If the limit value less than the threshold the limit_op will be opened. Added by michealxu</description>
    </property>
    
    <property>
        <name>mapred.child.java.opts</name>
        <value>-Xmx2048M</value> 
        <description>Larger heap-size for child jvms of maps/reduces.</description>
    </property>

    <property>
        <name>hive.service.newlogpath</name>
        <value>/usr/local/thive/log/session</value>
        <description>This string tells where the log will write. Added by michealxu</description>
    </property>

    <property>
        <name>mapred.reduce.slowstart.completed.maps</name>
        <value>0.9</value>
        <description>Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.
        </description>
    </property>

    <property>
      <name>tolerate.dataerror.write</name>
      <value>tolerate</value>
      <description>tolerate level: never, delete, tolerate, added by steventian</description>
    </property>
    
    <property>
      <name>tolerate.dataerror.readext</name>
      <value>tolerate</value>
      <description>tolerate level: never, delete, tolerate, added by steventian</description>
    </property>
    
    <property>
      <name>tolerate.numsplitechar.lessthan.numberfields.exread</name>
      <value>tolerate</value>
      <description>tolerate level: never, delete, tolerate, added by steventian</description>
    </property>
    
    <property>
      <name>fetch.execinfo.mode</name>
      <value>part</value>
      <description>fetch.execinfo.mode: no, part, all, added by steventian</description>
    </property>
    
    <property>
      <name>maxnum.rowdeleted.printlog.pertask</name>
      <value>100</value>
      <description>only print limited rows to avoid printing too many row and generating huge log file, in default 100 max error rows to be printed is recommended
      </description>
    </property>

    <property>
      <name>hive.formatstorage.limit.fix</name>
      <value>true</value>
      <description>set true to support the format storage row number limit fix</description>
    </property>

    <property>
      <name>hive.formatstorage.limit.num</name>
      <value>100000000</value>
      <description>set the user define format storage limit num, min is 100000000, max is 2000000000, default is 100000000</description>
    </property>

    <property>
      <name>hive.alterschema.activate.exttable</name>
      <value>false</value>
      <description>set hive.alterschema.activate.exttable false</description>
    </property>

    <property>
      <name>hadoop.job.ugi</name>
      <value>tdwadmin,users,dialout</value>
      <description>set hadoop ugi</description>
    </property>

<property>
   <name>mapred.map.tasks.speculative.execution</name>
   <value>true</value>
   <description>fs.default.name</description>
</property>

<property>
   <name>mapred.reduce.tasks.speculative.execution</name>
   <value>false</value>
   <description>fs.default.name</description>
</property>

<property>
  <name>hive.outerjoin.supports.filters</name>
  <value>true</value>
  <description>Added by Brantzhang for HIVE-1534, which can turn on or off for outer join filter push down.</description>
</property>

<property>
  <name>hive.join.nullbug.fix</name>
  <value>false</value>
  <description>a switch to determine whether fix the join null bug</description>
</property>

<property>
  <name>tdw.support.locking.modify.partitions</name>
  <value>true</value>
  <description>Whether tdw supports locking when add/drop partitions. A zookeeper instance must be up and running for the default hive lock manager to support read-write locks.</description>
</property>

<property>
  <name>tdw.lock.numretries</name>
  <value>40</value>
  <description>The number of times you want to try to get all the locks</description>
</property>

<property>
  <name>tdw.lock.sleep.between.retries</name>
  <value>3000</value>
  <description>The sleep time (in ms) between various retries</description>
</property>

<property>
  <name>tdw.zookeeper.quorum</name>
  <value>tl-zk-nd1:2181,tl-zk-nd2:2181,tl-zk-nd3:2181,tl-zk-nd4:2181,tl-zk-nd5:2181</value>
  <description>The list of zookeeper servers to talk to. This is only needed for read/write locks.</description>
</property>

<property>
  <name>tdw.zookeeper.session.timeout</name>
  <value>20000</value>
  <description>Zookeeper client's session timeout. The client is disconnected, and as a result, all locks released, if a heartbeat is not sent in the timeout.</description>
</property>

<property>
  <name>tdw.zookeeper.namespace</name>
  <value>tl-tdwlocks</value>
  <description>The parent node under which all zookeeper nodes are created.</description>
</property>

<property>
   <name>analysisbuffer.tmp.addr</name>
   <value>{{analysisbuffer_tmp_addr}}</value>
   <description>analysisbuffer local dir</description>
</property>


<property>  
  <name>tdw.ide.data.export.data.large</name>
  <value>true</value>
  <description>tdw.ide.data.export.data.large</description>
</property>  
<property>
  <name>tdw.ide.data.export.records</name>
  <value>1000000000</value>
  <description>tdw.ide.data.export.records</description>
</property>

<property>
  <name>hive.ql.showtblsize</name>
  <value>true</value>
  <description>tdw.ide.data.export.records</description>
</property>
<property>
  <name>hive.ql.showdbsize</name>
  <value>false</value>
  <description>tdw.ide.data.export.records</description>
</property>
<property>
  <name>dbexternal.table.fetchsize</name>
  <value>50</value>
  <description>tdw.ide.data.export.records</description>
</property>

<property>
  <name>hive.dividezero.returnnull</name>
  <value>true</value>
  <description>if the value is true, it will return null when divided by zero like mysql</description>
</property>

<property>
  <name>hive.default.fileformat</name>
  <value>rcfile</value>
  <description>default fileformat  should set to formatfile</description>
</property>
<property>
  <name>hive.default.formatcompress</name>
  <value>true</value>
  <description>format default compress</description>
</property>

<property>
  <name>hive.optimize.skewjoin</name>
  <value>false</value>
  <description>the switch to  skew join </description>
</property>
<property>
  <name>hive.skewjoin.key</name>
  <value>500000</value>
  <description>the count of one key over this value ,we think it is a big key</description>
</property>
<property>
  <name>hive.rctmpfile.path</name>
  <value>{{hive_rctmpfile_path}}</value>
  <description>this is the tmp dir for RowContainer's tmp files</description>
</property>
<property>
  <name>hive.rctmpfile.split.threshold</name>
  <value>800000</value>
  <description>the count of the big key over this value,we will split it more </description>
</property>

<property>
  <name>hive.rctmpfile.split.size</name>
  <value>400000</value>
  <description>it is the value of the records in one RowCountainer</description>
</property>

<property>
  <name>tdw.udf.switch.nopr.rules</name>
  <value>false</value>
  <description>the switch to the rules of operators </description>
</property>

<property>
  <name>hive.merge.inputfiles.rerange</name>
  <value>false</value>
  <description>rerange the split blocks true is default</description>
</property>

<property>
  <name>hive.merge.inputfiles.maxFileNumPerSplit</name>
  <value>1000000</value>
  <description>1000 is default</description>
</property>

<property>
  <name>hive.textline.seperator.mode</name>
  <value>1</value>
  <description>line seperator : 0 means all of CR LF and CRLF are line seperators,  1 means only LF and CRLF are line seperator, but CR is not. </description>
</property>

<property>
  <name>rowcontainer.block.minsize</name>
  <value>100</value>
  <description>TDW QE V1.0R021</description>
</property>

<property>
  <name>hive.exec.show.job.failure.debug.info</name>
  <value>true</value>
  <description>output MR error information</description>
</property>

<property>
  <name>hive.bi.open</name>
  <value>true</value>
  <description>open the function</description>
</property>
<property>
  <name>hive.bi.ip</name>
  <value>{{pg_server_hosts}}</value>
  <description>ip of pg</description>
</property>
<property>
  <name>hive.bi.port</name>
  <value>5432</value>
  <description>port of pg</description>
</property>
<property>
  <name>hive.bi.db</name>
  <value>tdw_query_info</value>
  <description>db name of pg</description>
</property>
<property>
  <name>hive.bi.user</name>
  <value>postgres</value>
  <description>user name of pg</description>
</property>
<property>
  <name>hive.bi.pwd</name>
  <value>postgres</value>
  <description>user password of pg</description>
</property>
<property>
  <name>hive.insert.part.support</name>
  <value>true</value>
  <description>supoort insert partition or not</description>
</property>

<property>
  <name>hive.insert.part.error.limit</name>
  <value>500</value>
  <description>error data limit per map</description>
</property>

<property>
  <name>hive.rowcontainer.get.multi.rctmp.path</name>
  <value>true</value>
  <description>supoort multi rctmp path or not</description>
</property>
<property>
  <name>hive.multi.rctmpfile.path.number</name>
  <value>{{hive_multi_rctmpfile_path_number}}</value>
  <description>numbers of rctmpfile path </description>
</property>
<property>
  <name>hive.multi.rctmpfile.path</name>
<value>{{hive_multi_rctmpfile_path}}</value>
  <description> rctmpfile path list </description>
</property>
<property>
  <name>tdw.stored.pgdata</name>
  <value>false</value>
  <description>supoort stored as pgdata or not</description>
</property>
<property>
  <name>pg_url</name>
  <value>tdwdata-bi-tdw:5432/tdwdata</value>
  <description>pg server, port and database name </description>
</property>
<property>
  <name>hive.inputfiles.splitbylinenum</name>
  <value>false</value>
  <description>supoort split by linenum or not</description>
</property>
<property>
  <name>hive.inputfiles.line_num_per_split</name>
  <value>1000000 </value>
  <description>how many line in one split </description>
</property>
<property>
  <name>hive.exec.parallel</name>
  <value>false</value>
  <description>execute job in parallel or not</description>
</property>
<property>
  <name>hive.exec.parallel.threads</name>
  <value>8</value>
  <description>how many line in one split </description>
</property>

<property>
  <name> hive.max.sql.length</name>
  <value>0</value>
  <description>limit sql length </description>
</property>

<property>
  <name>hive.session.timeout</name>
  <value>0</value>
  <description>hive session timeout </description>
</property>

<property>
  <name>hive.client.connection.limit</name>
  <value>true</value>
  <description>limit client number or not </description>
</property>

<property>
  <name>hive.client.connection.limit.number</name>
  <value>200</value>
  <description> client limit number </description>
</property>

<property>
  <name>hive.metadata.partition.update.optimizer</name>
  <value>true</value>
  <description> </description>
</property>

<property>
  <name>hive.metadata.partition.update.tolerate</name>
  <value>0</value>
  <description></description>
</property>

<property>
  <name>hive.metadata.partition.gabarge.collection</name>
  <value>false</value>
  <description></description>
</property>

<property>
  <name> hive.ql.openwith.optimize</name>
  <value>true</value>
  <description>limit sql length </description>
</property>

<property>
  <name>hive.metadata.list.partition.defaultislast</name>
  <value>false</value>
  <description></description>
</property>

<property>
  <name>hive.semantic.analyzer.optimizer.level</name>
  <value>2</value>
  <description> </description>
</property>

<property>
  <name>hive.bi.rownum.per.insert</name>
  <value>10000</value>
  <description> </description>
</property>

<property>
  <name>multihdfs.function.enable</name>
  <value>true</value>
  <description> </description>
</property>

<property>
  <name>select.max.limit</name>
  <value>20000</value>
  <description> the number of partitions when show create table </description>
</property>

<property>
  <name>mapreduce.reduce.shuffle.maxfetchfailures</name>
  <value>2</value>
  <description> the number of partitions when show create table </description>
</property>

<property>
  <name>mapred.job.per.map.maxfetchfailures</name>
  <value>2</value>
  <description> the number of partitions when show create table </description>
</property>

<property>
  <name>hive.errordata.process.level</name>
  <value>0</value>
  <description> 0: allow errordata into tdw; 1: throw away error data 2:when error data, return error </description>
</property>

<property>
  <name>bi.insert.new</name>
  <value>true</value>
  <description>use the new BI insert function or not</description>
</property>

<property>
  <name>bi.insert.tolerent</name>
  <value>false</value>
  <description>insert error tolerant or not</description>
</property>

<property>
  <name>bi.insert.tolerent.num</name>
  <value>10</value>
  <description> insert error tolerant number</description>
</property>

<property>
  <name>client.wait</name>
  <value>false</value>
  <description> close wait</description>
</property>

<property>
  <name>hive.insert.part.support</name>
  <value>true</value>
  <description>support weixin insert partition</description>
</property>

<property>
     <name>hive.support.external.partition</name>
     <value>true</value>
     <description>hive是否支持外表分区，true为支持，false为不支持</description>
</property>

<property>
     <name>hive.pb.badfile.skip</name>
     <value>true</value>
     <description>hive是否支持过滤错误pb文件，true为支持，false为不支持</description>
</property>
<property>
     <name>hive.pb.badfile.limit</name>
     <value>10</value>
     <description>pb错误文件的阀值，当超过这个阀值后报错</description>
</property>

<property>
    <name>hive.pg.timeout</name>
    <value>10</value>
    <description>hive连接pg时超时时间，单位为s，默认为10s</description>
</property>

<property>
    <name>hive.exec.estdistinct.bucketsize.log</name>
    <value>16</value>
    <description>hash桶大小</description>
</property>

<property>
    <name>hive.exec.estdistinct.buffsize.log</name>
    <value>8</value>
    <description>内存块大小</description>
</property>

<property>
     <name>lz.etl.flag_server</name>
     <value>ok_tdw_2034</value>
     <description>洛子出入库任务标志</description>
</property>

<property>
        <name>fs.default.name</name>
        <value>{{fs_default_name}}</value>
</property>

<property>
        <name>fs.defaultFS</name>
        <value>{{fs_default_name}}</value>
</property>

<property>
  <name>mapreduce.job.max.split.locations</name>
  <value>100000</value>
  <description>need for hadoop 2.x</description>
</property>

<property>
  <name>hive.protobuf.version</name>
  <value>2.5.0</value>
</property>

<property>
  <name>mapreduce.map.memory.mb</name>
  <value>2304</value>
  <description> Map任务的内存限制（需要根据实际情况微调）</description>
</property>


<property>
  <name>mapreduce.reduce.memory.mb</name>
  <value>3096</value>
  <description>Reduce任务的内存限制（需要根据实际情况微调）</description>
</property>


<property>
  <name>mapred.map.child.java.opts</name>
  <value>-Xmx2048M</value>
  <description>Map任务的堆内存大小（需要根据实际情况微调）</description>
</property>


<property>
  <name>mapred.reduce.child.java.opts</name>
  <value>-Xmx2816M</value>
  <description>Reduce任务的堆内存大小（需要根据实际情况微调）</description>
</property>


<property>
  <name>hive.adjust.resource.enbale</name>
  <value>true</value>
  <description>是否开启根据job复杂度调整内存的功能</description>
</property>



<property>
  <name>hive.simple.map.container.memory.mb</name>
  <value>1280</value>
  <description>当开启了"hive.adjust.resource.enbale"开关,时 不包含复杂算子的map任务堆内存占用的内存（需要根据实际情况微调）</description>
</property>


<property>
  <name>hive.simple.map.memory.mb</name>
  <value>1024</value>
  <description>当开启了"hive.adjust.resource.enbale"开关,时 不包含复杂算子的map任务堆内存占用的内存（需要根据实际情况微调）</description>
</property>


<property>
  <name>mapreduce.job.split.metainfo.maxsize</name>
  <value>50000000</value>
</property>

<!--
<property>
  <name>mapred.cache.archives</name>
  <value>hdfs://tl-teg-nn-tdw.tencent-distribute.com:54310/user/DistributedCache/hive/libjars/hadoop-gpl-compression-0.1.0.jar,hdfs://tl-teg-nn-tdw.tencent-distribute.com:54310/user/DistributedCache/hive/libjars/protobufRIO.jar</value>
  <description></description>
</property>
-->

<property>
  <name>hive.optimize.ppr.in.enable</name>
  <value>true</value>
</property>

<property>
  <name>hive.optimize.ppr.rangepart.new.enable</name>
  <value>true</value>
</property>

<property>
  <name>hive.exttable.createdir.ifnotexist</name>
  <value>true</value>
</property>

<property>
  <name>dfs.replication</name>
  <value>2</value>
</property>

<property>
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
</property>

<property>
  <name>io.compression.codec.lzo.class</name>
  <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>

<property>
  <name>hive.udtf.explode.change0size2null</name>
  <value>false</value>
</property>


<property>
  <name>io.file.buffer.size</name>
  <value>65536</value>
</property>

<property>
  <name>fs.inmemory.size.mb</name>
  <value>512</value>
</property>

<property>
  <name>mapreduce.task.io.sort.mb</name>
  <value>320</value>
</property>

<property>
  <name>mapreduce.task.io.sort.factor</name>
  <value>120</value>
</property>

<property>
  <name>mapreduce.map.output.compress</name>
  <value>true</value>
</property>

<property>
  <name>mapreduce.map.output.compress.codec</name>
  <value>org.apache.hadoop.io.compress.LzoCodec</value>
</property>

<property>
  <name>hive.udtf.explode.change_null2null</name>
  <value>false</value>
</property>


<property>
  <name>hive.metastore.master.urls</name>
  <value></value>
</property>
    
<property>
  <name>hive.metastore.slave.urls.</name>
  <value></value>
</property>

<property>
  <name>hive.metastore.slave.urls.</name>
  <value></value>
</property>


<property>
  <name>hive.metastore.slave.urls.</name>
  <value></value>
</property>


<property>
  <name>hive.metastore.master.read</name>
  <value>true</value>
</property>

<property>
  <name>hive.metastore.readwrite.split</name>
  <value>false</value>
</property>




<property>
  <name>hive.exttable.location.auth</name>
  <value>false</value>
</property>


<property>
  <name>hive.bi.interval.time</name>
  <value>1000</value>
  <description>interval time to operate on monitor db, unit: ms</description>
</property>
<property>
  <name>hive.bi.exec.sqls.number</name>
  <value>64</value>
  <description>execute sqls number once operation on monitor db</description>
</property>
<property>
  <name>hive.bi.queue.sqls.threshold</name>
  <value>80</value>
  <description>when queue's length is over than this value, operator monitor db without sleep</description>
</property>
<property>
  <name>hive.bi.queue.sqls.threshold.number</name>
  <value>80</value>
  <description>without sleeping execute sqls number once time</description>
</property>


<property>
  <name>hive.metastore.global.master.read</name>
  <value>true</value>
</property>

<property>
  <name>hive.metastore.global.master.urls</name>
  <value></value>
</property>

<property>
  <name>hive.metastore.global.slave.urls.meta-g-bi-tdw.tencent-distribute.com:5432</name>
  <value></value>
</property>

</configuration>
